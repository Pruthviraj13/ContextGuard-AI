# ContextGuard AI

ContextGuard AI is a production-grade Retrieval-Augmented Generation
(RAG) backend built with FastAPI.\
It delivers grounded, reliable AI responses using selective retrieval,
strict validation, and production-focused backend design.

------------------------------------------------------------------------

## ğŸš€ Key Features

-   ğŸ” JWT-based Authentication
-   ğŸ§  Intelligent Query Classification (RAG vs Non-RAG routing)
-   ğŸ“š FAISS Vector Index for semantic search
-   ğŸ›¡ Confidence-based Answer Validation (anti-hallucination guardrail)
-   ğŸ” Retry & Fallback Strategy for LLM failures
-   ğŸ“Š Structured Logging & Observability (latency, request IDs)
-   ğŸ—‘ Soft Delete & Document Lifecycle Management
-   âš¡ Async FastAPI backend (ASGI-based)
-   ğŸ§© Streamlit lightweight testing UI

------------------------------------------------------------------------

## ğŸ— Architecture Overview

User â†’ Streamlit UI â†’ FastAPI Backend â†’ Classifier â†’ (RAG or Direct LLM)
â†’ FAISS Retrieval â†’ LLM Generation â†’ Validation Layer â†’ Response

------------------------------------------------------------------------

## ğŸ“¦ Project Structure

app/ â”‚ â”œâ”€â”€ api/ â”‚ â””â”€â”€ v1/ â”‚ â”œâ”€â”€ endpoints/ â”‚ â”‚ â”œâ”€â”€ auth.py â”‚ â”‚ â”œâ”€â”€
rag.py â”‚ â”‚ â”œâ”€â”€ ingest.py â”‚ â”‚ â””â”€â”€ health.py â”‚ â””â”€â”€ router.py â”‚ â”œâ”€â”€
services/ â”‚ â”œâ”€â”€ classifier.py â”‚ â”œâ”€â”€ generator.py â”‚ â”œâ”€â”€ evaluator.py â”‚
â””â”€â”€ retriever.py â”‚ â”œâ”€â”€ core/ â”‚ â”œâ”€â”€ logging.py â”‚ â”œâ”€â”€
rate_limit_middleware.py â”‚ â””â”€â”€ request_context.py â”‚ â””â”€â”€ main.py

------------------------------------------------------------------------

## ğŸ” How It Works

1.  User sends a question.
2.  Query classifier determines if RAG is required.
3.  If required:
    -   Relevant document chunks are retrieved from FAISS.
    -   LLM generates answer using retrieved context only.
    -   Evaluator validates response and assigns confidence.
4.  If confidence is low â†’ Safe fallback message returned.
5.  All requests logged with latency and request IDs.

------------------------------------------------------------------------

## ğŸ§  Why This Project Matters

-   Prevents hallucinations instead of hiding them.
-   Uses selective retrieval to reduce cost and latency.
-   Handles LLM failures gracefully.
-   Designed with production reliability in mind.
-   Demonstrates backend + GenAI integration depth.

------------------------------------------------------------------------

## ğŸ›  How to Run the Application

### 1ï¸âƒ£ Clone the Repository

git clone `<your-repo-url>`{=html} cd contextguard-ai

------------------------------------------------------------------------

### 2ï¸âƒ£ Create Virtual Environment

python -m venv venv

Activate:

Windows: venv`\Scripts`{=tex}`\activate`{=tex}

Mac/Linux: source venv/bin/activate

------------------------------------------------------------------------

### 3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

------------------------------------------------------------------------

### 4ï¸âƒ£ Set Environment Variables

Windows (PowerShell): setx GROQ_API_KEY "your_groq_api_key" setx
GROQ_MODEL "llama-3.1-8b-instant"

Mac/Linux: export GROQ_API_KEY="your_groq_api_key" export
GROQ_MODEL="llama-3.1-8b-instant"

Restart terminal after setting environment variables.

------------------------------------------------------------------------

### 5ï¸âƒ£ Run Backend

uvicorn app.main:app --reload

Access API Docs: http://127.0.0.1:8000/docs

------------------------------------------------------------------------

### 6ï¸âƒ£ Run Streamlit UI (Optional)

streamlit run ui.py

------------------------------------------------------------------------

## ğŸ“ˆ Example API Flow

1.  POST /api/v1/auth/login â†’ Get JWT
2.  POST /api/v1/ingest â†’ Upload document
3.  POST /api/v1/query â†’ Ask question (with Bearer token)

------------------------------------------------------------------------

## ğŸ›¡ Production Design Considerations

-   Retry & fallback model strategy
-   Prompt versioning
-   Confidence-based rejection
-   Soft deletes instead of vector mutation
-   Configurable model selection via environment variables

------------------------------------------------------------------------

## ğŸ“Œ Future Enhancements

-   Hybrid Search (BM25 + Vector)
-   Multi-tenant isolation
-   Offline evaluation dataset
-   Cost tracking dashboard

------------------------------------------------------------------------

## ğŸ“„ License

For educational and demonstration purposes.
